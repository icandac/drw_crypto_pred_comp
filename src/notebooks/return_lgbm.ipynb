{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4faf7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc, os, psutil\n",
    "from typing import List, Tuple\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from joblib import dump\n",
    "import warnings\n",
    "from src.utils.light_preprocess import Preprocessor\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "input_folder = '../../data/'\n",
    "output_folder = '../../outputs/'\n",
    "seed = 42\n",
    "n_splits = 5\n",
    "\n",
    "def mem_mb() -> float:\n",
    "    \"\"\"Return current RSS in megabytes.\"\"\"\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f7db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_train_raw' (DataFrame)\n",
      "Stored 'df_test_raw' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_parquet(f'{input_folder}train.parquet')\n",
    "df_test_raw = pd.read_parquet(f'{input_folder}test.parquet')\n",
    "%store df_train_raw\n",
    "%store df_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af2acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_filter(\n",
    "    df: pd.DataFrame, target_col: str = \"label\", thresh_ratio: float = 0.1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove feature columns whose variance is < `thresh_ratio` x var(label).\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Training frame (label + features).\n",
    "        thresh_ratio : float\n",
    "            Keep features whose var >= thresh_ratio * var(label).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    label_var = df[target_col].var()\n",
    "    thresh = label_var * thresh_ratio\n",
    "\n",
    "    keep_cols = [target_col] + [\n",
    "        c for c in df.columns if c != target_col and df[c].var() >= thresh\n",
    "    ]\n",
    "    pruned = df[keep_cols].copy()\n",
    "    print(f\"Variance filter kept {len(keep_cols)-1} of {df.shape[1]-1} columns\")\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a178478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icandac/Documents/drw_crypto_pred_comp/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance filter kept 855 of 895 columns\n"
     ]
    }
   ],
   "source": [
    "df_train = variance_filter(df_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df96138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_cluster_select(\n",
    "    df: pd.DataFrame, target: str = \"label\", thresh: float = 0.90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Target-aware correlation-clustering filter.\n",
    "\n",
    "    Keeps at most **one** feature from every group of highly-correlated\n",
    "    columns (|corr| ≥ `thresh`).  For each group the survivor is the\n",
    "    feature with the strongest absolute correlation to the target.\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Input frame that still contains `target`.\n",
    "        thresh : float\n",
    "            Absolute Pearson correlation threshold that triggers grouping\n",
    "            (default 0.90).\n",
    "    Returns:\n",
    "        DataFrame\n",
    "            Same rows, but with duplicates pruned.\n",
    "    \"\"\"\n",
    "    feats = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    corr = feats.astype(\"float32\").corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    upper = corr.where(mask)\n",
    "\n",
    "    survivors: List[str] = []\n",
    "    processed = set()  # columns already clustered\n",
    "\n",
    "    for col in upper.columns:\n",
    "        if col in processed:\n",
    "            continue\n",
    "\n",
    "        # all features strongly correlated with `col`\n",
    "        cluster = upper.index[upper[col] >= thresh].tolist()\n",
    "        cluster.append(col)\n",
    "\n",
    "        processed.update(cluster)\n",
    "\n",
    "        # pick the column with largest |corr| to the label\n",
    "        best = feats[cluster].corrwith(y).abs().idxmax()\n",
    "        survivors.append(best)\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    survivors = list(dict.fromkeys(survivors))\n",
    "\n",
    "    kept_df = df[[target] + survivors].copy()\n",
    "    print(\n",
    "        f\"Correlation filter kept {len(survivors)} \"\n",
    "        f\"of {feats.shape[1]} columns (thresh={thresh})\"\n",
    "    )\n",
    "    return kept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593e663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation filter kept 630 of 855 columns (thresh=0.9)\n"
     ]
    }
   ],
   "source": [
    "df_train = corr_cluster_select(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfd9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_train' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f12c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"label\",\n",
    "    n_splits: int = 5,\n",
    "    seed: int = 42,\n",
    "    importance_type: str = \"gain\",  # \"gain\" or \"split\",\n",
    "    var_ratio: float = 0.1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Train LightGBM on each fold, average feature importances, and\n",
    "    return a ranked table.\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Raw training frame (timestamp index, features + label).\n",
    "        target_col : str\n",
    "            Column name of the regression target.\n",
    "        n_splits : int\n",
    "            TimeSeriesSplit folds.\n",
    "        importance_type : str\n",
    "            \"gain\" - total gain of splits (default, more robust)\n",
    "            \"split\" - number of times the feature is used in splits.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [feature, importance] sorted descending.\n",
    "    \"\"\"\n",
    "    y = df[target_col]\n",
    "    x = df.drop(columns=[target_col])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    imp_accum = pd.Series(0.0, index=x.columns)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"regression\",\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=256,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        seed=seed,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(x), 1):\n",
    "        print(f\"Training fold {fold}/{n_splits} …\", end=\"\\r\")\n",
    "\n",
    "        x_tr = x.iloc[tr_idx].astype(\"float32\")\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        x_val = x.iloc[val_idx].astype(\"float32\")\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        tr_ds = lgb.Dataset(x_tr, y_tr, free_raw_data=False)\n",
    "        val_ds = lgb.Dataset(x_val, y_val, reference=tr_ds, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            tr_ds,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[val_ds],\n",
    "            callbacks=[lgb.early_stopping(200)],\n",
    "        )\n",
    "\n",
    "        imp_accum += pd.Series(\n",
    "            model.feature_importance(importance_type=importance_type),\n",
    "            index=x.columns,\n",
    "            dtype=\"float64\",\n",
    "        )\n",
    "\n",
    "        del model, tr_ds, val_ds, x_tr, x_val  # <── release memory now\n",
    "        gc.collect()\n",
    "\n",
    "    imp_df = (\n",
    "        imp_accum.div(n_splits)\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"feature\", 0: \"importance\"})\n",
    "    )\n",
    "\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629643cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open this if variance and feature correlation are not used\n",
    "# df_train = df_train_raw.copy()\n",
    "# df_test = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8c8d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.960421\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.02821\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.04097\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.943593\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.14101\n",
      "Top 48 features:\n",
      "    feature   importance\n",
      "0     X686  2562.972731\n",
      "1     X748  2480.580946\n",
      "2     X466  2360.977019\n",
      "3     X862  2200.282824\n",
      "4     X885  2184.692510\n",
      "5     X873  1933.185632\n",
      "6     X344  1620.960222\n",
      "7     X537  1586.908585\n",
      "8      X86  1512.748166\n",
      "9     X752  1469.943217\n",
      "10    X786  1427.386214\n",
      "11    X272  1411.428012\n",
      "12    X465  1407.638287\n",
      "13    X198  1362.122412\n",
      "14    X295  1314.388788\n",
      "15     X95  1305.104245\n",
      "16    X422  1263.198605\n",
      "17    X283  1221.743762\n",
      "18    X852  1115.226031\n",
      "19    X586  1108.722202\n",
      "20    X804  1088.629401\n",
      "21    X695  1074.736162\n",
      "22     X98  1060.546739\n",
      "23    X530  1021.055627\n",
      "24    X842   972.973193\n",
      "25    X889   967.481166\n",
      "26    X604   960.596179\n",
      "27    X890   954.043259\n",
      "28     X90   932.773407\n",
      "29    X385   929.577789\n",
      "30    X611   925.800827\n",
      "31    X674   882.876010\n",
      "32    X856   878.871808\n",
      "33    X219   874.663399\n",
      "34    X540   863.289246\n",
      "35    X598   855.271593\n",
      "36    X136   854.333191\n",
      "37    X866   853.631145\n",
      "38    X428   814.931693\n",
      "39    X278   784.176987\n",
      "40    X373   758.994403\n",
      "41    X335   757.794025\n",
      "42    X683   744.817792\n",
      "43    X384   734.608005\n",
      "44     X92   722.684808\n",
      "45    X138   705.990184\n",
      "46    X345   683.364458\n",
      "47    X126   681.716780\n"
     ]
    }
   ],
   "source": [
    "# No need to run this if you already have the feature importance\n",
    "# from a previous run, just load it instead.\n",
    "# imp_df = pd.read_csv(f\"{output_folder}feature_importance.csv\")\n",
    "imp_df = compute_feature_importance(\n",
    "    df_train, target_col=\"label\", n_splits=5, var_ratio=0.1\n",
    ")\n",
    "imp_df.to_csv(f\"{output_folder}feature_importance.csv\", index=False)\n",
    "top_n = 48  # determined by optimize_topn_features with optuna method previously\n",
    "print(f\"Top {top_n} features:\\n\", imp_df.head(int(top_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7262e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_reduced' (DataFrame)\n",
      "Stored 'test_reduced' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "cols_to_expand = imp_df.head(top_n)[\"feature\"].tolist()\n",
    "train_reduced = df_train_raw[[\"label\", *cols_to_expand, \"volume\", \"sell_qty\", \"buy_qty\"]].copy()\n",
    "test_reduced = df_test_raw[[*cols_to_expand, \"volume\", \"sell_qty\", \"buy_qty\"]].copy()\n",
    "# x_train = train_reduced.drop(columns=[\"label\"])\n",
    "# y_train = df_train[\"label\"]\n",
    "%store train_reduced\n",
    "%store test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b50219b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a one-step-ahead LOG-RETURN target\n",
    "def make_return_target(df: pd.DataFrame, label_col: str = \"label\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    y_t = log(price_{t+1}) - log(price_t)\n",
    "    (shift(-1) aligns *current* features with *next* return)\n",
    "    \"\"\"\n",
    "    return np.log(df[label_col]).diff().shift(-1)\n",
    "\n",
    "\n",
    "# Cross-validation that returns OoF predictions *and* fold models\n",
    "def cv_lightgbm_returns(x: pd.DataFrame,\n",
    "                        y_ret: pd.Series,\n",
    "                        n_splits: int = 5,\n",
    "                        seed: int = 42):\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    oof_pred = np.full(len(y_ret), np.nan, dtype=np.float32)\n",
    "    models = []\n",
    "\n",
    "    lgb_params = dict(\n",
    "        objective=\"regression\",\n",
    "        metric=\"None\",               # we supply Pearson ourselves\n",
    "        boosting_type=\"gbdt\",\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=256,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        seed=seed,\n",
    "        verbose=-1,\n",
    "        num_threads=4,\n",
    "    )\n",
    "\n",
    "    def lgb_pearson(preds, train_data):\n",
    "        r = np.corrcoef(train_data.get_label(), preds)[0, 1]\n",
    "        return \"pearson\", r, True\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(x), 1):\n",
    "        x_tr, y_tr = x.iloc[tr_idx], y_ret.iloc[tr_idx]\n",
    "        x_val, y_val = x.iloc[val_idx], y_ret.iloc[val_idx]\n",
    "\n",
    "        m = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb.Dataset(x_tr, y_tr),\n",
    "            num_boost_round=4000,\n",
    "            valid_sets=[lgb.Dataset(x_val, y_val)],\n",
    "            feval=lgb_pearson,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(400, first_metric_only=True),\n",
    "                lgb.log_evaluation(400),\n",
    "            ],\n",
    "        )\n",
    "        models.append(m)\n",
    "\n",
    "        oof_pred[val_idx] = m.predict(x_val, num_iteration=m.best_iteration)\n",
    "        print(f\"fold {fold}: Pearson = {pearsonr(y_val, oof_pred[val_idx])[0]:.5f}\")\n",
    "\n",
    "        del x_tr, y_tr, x_val, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    mask = ~np.isnan(oof_pred)\n",
    "    oof_r = pearsonr(y_ret.iloc[mask], oof_pred[mask])[0]\n",
    "    print(f\"OOF Pearson (whole series) = {oof_r:.5f}\")\n",
    "\n",
    "    best_iter = int(np.mean([m.best_iteration for m in models]))\n",
    "    return oof_pred, models, best_iter, oof_r\n",
    "\n",
    "\n",
    "# Re-train one final booster on *all* data, predict returns on test,\n",
    "# then reconstruct price-level forecasts\n",
    "def train_full_and_predict(models, best_iter,\n",
    "                           x_train, y_ret, x_test,\n",
    "                           last_train_price: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    last_train_price  - price at row T (the final row of the training period);\n",
    "                        needed to reconstruct price_t+1 from predicted return_t.\n",
    "    \"\"\"\n",
    "    full_ds = lgb.Dataset(x_train, y_ret)\n",
    "    full_model = lgb.train(models[0].params, full_ds,\n",
    "                           num_boost_round=best_iter)\n",
    "\n",
    "    ret_pred = full_model.predict(x_test)\n",
    "\n",
    "    # price_{t+1} = price_t * exp(return_t)\n",
    "    price_pred = np.empty_like(ret_pred)\n",
    "    p_prev = last_train_price\n",
    "    for i, r in enumerate(ret_pred):\n",
    "        p_next = p_prev * np.exp(r)\n",
    "        price_pred[i] = p_next\n",
    "        p_prev = p_next            # chain for multi-step test horizon\n",
    "\n",
    "    return price_pred\n",
    "\n",
    "\n",
    "# End-to-end helper you call from your notebook / script\n",
    "def run_return_lgbm(train_df: pd.DataFrame,\n",
    "                    test_df: pd.DataFrame,\n",
    "                    feature_cols: list | None = None):\n",
    "    \"\"\"\n",
    "    train_df  - your already-filtered frame that *still contains* 'label'\n",
    "    test_df   - same features (no label)\n",
    "    \"\"\"\n",
    "    print(\"building return target\")\n",
    "    y_ret = make_return_target(train_df).dropna()\n",
    "    train_df = train_df.loc[y_ret.index]          # drop first row (NaN return)\n",
    "\n",
    "    x_train = train_df.drop(columns=[\"label\"]) if feature_cols is None \\\n",
    "              else train_df[feature_cols]\n",
    "    x_test  = test_df[x_train.columns]            # keep same order\n",
    "\n",
    "    print(\"CV training\")\n",
    "    oof_pred, models, best_iter, _ = cv_lightgbm_returns(x_train, y_ret)\n",
    "\n",
    "    last_price = train_df[\"label\"].iloc[-1]\n",
    "    print(f\"full-data training (best_iter={best_iter}) and test prediction\")\n",
    "    price_pred = train_full_and_predict(models, best_iter,\n",
    "                                        x_train, y_ret, x_test,\n",
    "                                        last_price)\n",
    "\n",
    "    return price_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building return target\n",
      "CV training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icandac/Documents/drw_crypto_pred_comp/.venv/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's pearson: 0.0164427\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's pearson: 0.0197526\n",
      "Evaluated only: pearson\n",
      "fold 1: Pearson = 0.01975\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's pearson: 0.0149598\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's pearson: 0.0189252\n",
      "Evaluated only: pearson\n",
      "fold 2: Pearson = 0.01893\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's pearson: 0.0143096\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid_0's pearson: 0.0161513\n",
      "Evaluated only: pearson\n",
      "fold 3: Pearson = 0.01615\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's pearson: 0.00664923\n",
      "[800]\tvalid_0's pearson: 0.00550347\n",
      "Early stopping, best iteration is:\n",
      "[552]\tvalid_0's pearson: 0.00720419\n",
      "Evaluated only: pearson\n",
      "fold 4: Pearson = 0.00720\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[400]\tvalid_0's pearson: 0.00480404\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's pearson: 0.0108075\n",
      "Evaluated only: pearson\n",
      "fold 5: Pearson = 0.01081\n",
      "OOF Pearson (whole series) = 0.00672\n",
      "full-data training (best_iter=146) and test prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/yl4h4ppx62q421vssm841jyw0000gn/T/ipykernel_62111/4037241314.py:88: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  p_next = p_prev * np.exp(r)\n",
      "/var/folders/bp/yl4h4ppx62q421vssm841jyw0000gn/T/ipykernel_62111/3746879583.py:4: RuntimeWarning: overflow encountered in cast\n",
      "  sample[sample.columns[-1]] = price_pred.astype(np.float32)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "price_pred = run_return_lgbm(train_reduced, test_reduced)\n",
    "\n",
    "sample = pd.read_csv(f\"{input_folder}sample_submission.csv\")\n",
    "sample[sample.columns[-1]] = price_pred.astype(np.float32)\n",
    "sample.to_csv(f\"{output_folder}submission_return_lgbm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drw_crypto_pred_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
