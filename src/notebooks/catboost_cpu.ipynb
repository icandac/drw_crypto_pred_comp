{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4faf7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc, os, psutil\n",
    "from typing import List, Tuple\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from joblib import dump\n",
    "import warnings\n",
    "from src.utils.light_preprocess import Preprocessor\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "input_folder = '../../data/'\n",
    "output_folder = '../../outputs/'\n",
    "seed = 42\n",
    "n_splits = 5\n",
    "\n",
    "def mem_mb() -> float:\n",
    "    \"\"\"Return current RSS in megabytes.\"\"\"\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748f7db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_train_raw' (DataFrame)\n",
      "Stored 'df_test_raw' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_parquet(f'{input_folder}train.parquet')\n",
    "df_test_raw = pd.read_parquet(f'{input_folder}test.parquet')\n",
    "%store df_train_raw\n",
    "%store df_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af2acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_filter(\n",
    "    df: pd.DataFrame, target_col: str = \"label\", thresh_ratio: float = 0.1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove feature columns whose variance is < `thresh_ratio` x var(label).\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Training frame (label + features).\n",
    "        thresh_ratio : float\n",
    "            Keep features whose var >= thresh_ratio * var(label).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    label_var = df[target_col].var()\n",
    "    thresh = label_var * thresh_ratio\n",
    "\n",
    "    keep_cols = [target_col] + [\n",
    "        c for c in df.columns if c != target_col and df[c].var() >= thresh\n",
    "    ]\n",
    "    pruned = df[keep_cols].copy()\n",
    "    print(f\"Variance filter kept {len(keep_cols)-1} of {df.shape[1]-1} columns\")\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a178478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icandac/Documents/drw_crypto_pred_comp/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance filter kept 855 of 895 columns\n"
     ]
    }
   ],
   "source": [
    "df_train = variance_filter(df_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df96138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_cluster_select(\n",
    "    df: pd.DataFrame, target: str = \"label\", thresh: float = 0.90\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Target-aware correlation-clustering filter.\n",
    "\n",
    "    Keeps at most **one** feature from every group of highly-correlated\n",
    "    columns (|corr| ≥ `thresh`).  For each group the survivor is the\n",
    "    feature with the strongest absolute correlation to the target.\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Input frame that still contains `target`.\n",
    "        thresh : float\n",
    "            Absolute Pearson correlation threshold that triggers grouping\n",
    "            (default 0.90).\n",
    "    Returns:\n",
    "        DataFrame\n",
    "            Same rows, but with duplicates pruned.\n",
    "    \"\"\"\n",
    "    feats = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    corr = feats.astype(\"float32\").corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    upper = corr.where(mask)\n",
    "\n",
    "    survivors: List[str] = []\n",
    "    processed = set()  # columns already clustered\n",
    "\n",
    "    for col in upper.columns:\n",
    "        if col in processed:\n",
    "            continue\n",
    "\n",
    "        # all features strongly correlated with `col`\n",
    "        cluster = upper.index[upper[col] >= thresh].tolist()\n",
    "        cluster.append(col)\n",
    "\n",
    "        processed.update(cluster)\n",
    "\n",
    "        # pick the column with largest |corr| to the label\n",
    "        best = feats[cluster].corrwith(y).abs().idxmax()\n",
    "        survivors.append(best)\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    survivors = list(dict.fromkeys(survivors))\n",
    "\n",
    "    kept_df = df[[target] + survivors].copy()\n",
    "    print(\n",
    "        f\"Correlation filter kept {len(survivors)} \"\n",
    "        f\"of {feats.shape[1]} columns (thresh={thresh})\"\n",
    "    )\n",
    "    return kept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593e663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation filter kept 630 of 855 columns (thresh=0.9)\n"
     ]
    }
   ],
   "source": [
    "df_train = corr_cluster_select(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfd9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_train' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f12c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"label\",\n",
    "    n_splits: int = 5,\n",
    "    seed: int = 42,\n",
    "    importance_type: str = \"gain\",  # \"gain\" or \"split\",\n",
    "    var_ratio: float = 0.1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Train LightGBM on each fold, average feature importances, and\n",
    "    return a ranked table.\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame\n",
    "            Raw training frame (timestamp index, features + label).\n",
    "        target_col : str\n",
    "            Column name of the regression target.\n",
    "        n_splits : int\n",
    "            TimeSeriesSplit folds.\n",
    "        importance_type : str\n",
    "            \"gain\" - total gain of splits (default, more robust)\n",
    "            \"split\" - number of times the feature is used in splits.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns [feature, importance] sorted descending.\n",
    "    \"\"\"\n",
    "    y = df[target_col]\n",
    "    x = df.drop(columns=[target_col])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    imp_accum = pd.Series(0.0, index=x.columns)\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"regression\",\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=256,\n",
    "        feature_fraction=0.9,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        seed=seed,\n",
    "        verbose=-1,\n",
    "    )\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(tscv.split(x), 1):\n",
    "        print(f\"Training fold {fold}/{n_splits} …\", end=\"\\r\")\n",
    "\n",
    "        x_tr = x.iloc[tr_idx].astype(\"float32\")\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        x_val = x.iloc[val_idx].astype(\"float32\")\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        tr_ds = lgb.Dataset(x_tr, y_tr, free_raw_data=False)\n",
    "        val_ds = lgb.Dataset(x_val, y_val, reference=tr_ds, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            tr_ds,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[val_ds],\n",
    "            callbacks=[lgb.early_stopping(200)],\n",
    "        )\n",
    "\n",
    "        imp_accum += pd.Series(\n",
    "            model.feature_importance(importance_type=importance_type),\n",
    "            index=x.columns,\n",
    "            dtype=\"float64\",\n",
    "        )\n",
    "\n",
    "        del model, tr_ds, val_ds, x_tr, x_val  # <── release memory now\n",
    "        gc.collect()\n",
    "\n",
    "    imp_df = (\n",
    "        imp_accum.div(n_splits)\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"feature\", 0: \"importance\"})\n",
    "    )\n",
    "\n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629643cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open this if variance and feature correlation are not used\n",
    "# df_train = df_train_raw.copy()\n",
    "# df_test = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8c8d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.960421\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.02821\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.04097\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.943593\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.14101\n",
      "Top 48 features:\n",
      "    feature   importance\n",
      "0     X686  2562.972731\n",
      "1     X748  2480.580946\n",
      "2     X466  2360.977019\n",
      "3     X862  2200.282824\n",
      "4     X885  2184.692510\n",
      "5     X873  1933.185632\n",
      "6     X344  1620.960222\n",
      "7     X537  1586.908585\n",
      "8      X86  1512.748166\n",
      "9     X752  1469.943217\n",
      "10    X786  1427.386214\n",
      "11    X272  1411.428012\n",
      "12    X465  1407.638287\n",
      "13    X198  1362.122412\n",
      "14    X295  1314.388788\n",
      "15     X95  1305.104245\n",
      "16    X422  1263.198605\n",
      "17    X283  1221.743762\n",
      "18    X852  1115.226031\n",
      "19    X586  1108.722202\n",
      "20    X804  1088.629401\n",
      "21    X695  1074.736162\n",
      "22     X98  1060.546739\n",
      "23    X530  1021.055627\n",
      "24    X842   972.973193\n",
      "25    X889   967.481166\n",
      "26    X604   960.596179\n",
      "27    X890   954.043259\n",
      "28     X90   932.773407\n",
      "29    X385   929.577789\n",
      "30    X611   925.800827\n",
      "31    X674   882.876010\n",
      "32    X856   878.871808\n",
      "33    X219   874.663399\n",
      "34    X540   863.289246\n",
      "35    X598   855.271593\n",
      "36    X136   854.333191\n",
      "37    X866   853.631145\n",
      "38    X428   814.931693\n",
      "39    X278   784.176987\n",
      "40    X373   758.994403\n",
      "41    X335   757.794025\n",
      "42    X683   744.817792\n",
      "43    X384   734.608005\n",
      "44     X92   722.684808\n",
      "45    X138   705.990184\n",
      "46    X345   683.364458\n",
      "47    X126   681.716780\n"
     ]
    }
   ],
   "source": [
    "# No need to run this if you already have the feature importance\n",
    "# from a previous run, just load it instead.\n",
    "# imp_df = pd.read_csv(f\"{output_folder}feature_importance.csv\")\n",
    "imp_df = compute_feature_importance(\n",
    "    df_train, target_col=\"label\", n_splits=5, var_ratio=0.1\n",
    ")\n",
    "imp_df.to_csv(f\"{output_folder}feature_importance.csv\", index=False)\n",
    "top_n = 48  # determined by optimize_topn_features with optuna method previously\n",
    "print(f\"Top {top_n} features:\\n\", imp_df.head(int(top_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7262e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_reduced' (DataFrame)\n",
      "Stored 'test_reduced' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "cols_to_expand = imp_df.head(top_n)[\"feature\"].tolist()\n",
    "train_reduced = df_train_raw[[\"label\", *cols_to_expand, \"volume\", \"sell_qty\", \"buy_qty\"]].copy()\n",
    "test_reduced = df_test_raw[[*cols_to_expand, \"volume\", \"sell_qty\", \"buy_qty\"]].copy()\n",
    "# x_train = train_reduced.drop(columns=[\"label\"])\n",
    "# y_train = df_train[\"label\"]\n",
    "%store train_reduced\n",
    "%store test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928e3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(preds, sample_path=f\"{input_folder}sample_submission.csv\",\n",
    "                    out_path=f\"{output_folder}submission_cat.csv\"):\n",
    "    sub = pd.read_csv(sample_path)\n",
    "    assert len(sub) == len(preds)\n",
    "    sub[sub.columns[-1]] = preds.astype(np.float32)\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06a1018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (525887, 51) Test shape: (538150, 51) RAM: 2151.890944 MB\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [c for c in train_reduced.columns if c not in [\"label\"]]\n",
    "cat_cols = []      # or [] if not present\n",
    "\n",
    "x_train = train_reduced[feature_cols]\n",
    "y_train = train_reduced[\"label\"]\n",
    "x_test  = test_reduced.copy()\n",
    "\n",
    "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape,\n",
    "      \"RAM:\", mem_mb(), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff3d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0121026\ttest: 0.9796283\tbest: 0.9796283 (0)\ttotal: 83.9ms\tremaining: 11m 11s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9796282856\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "fold 1: Pearson = 0.00236\n",
      "0:\tlearn: 0.9955341\ttest: 1.0054654\tbest: 1.0054654 (0)\ttotal: 22.5ms\tremaining: 2m 59s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 1.005465418\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "fold 2: Pearson = 0.02352\n",
      "0:\tlearn: 0.9973528\ttest: 1.0098668\tbest: 1.0098668 (0)\ttotal: 22.9ms\tremaining: 3m 3s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 1.008785554\n",
      "bestIteration = 3\n",
      "\n",
      "Shrink model to first 4 iterations.\n",
      "fold 3: Pearson = 0.03324\n",
      "0:\tlearn: 0.9996072\ttest: 0.9753651\tbest: 0.9753651 (0)\ttotal: 27.2ms\tremaining: 3m 37s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9739470073\n",
      "bestIteration = 8\n",
      "\n",
      "Shrink model to first 9 iterations.\n",
      "fold 4: Pearson = 0.08467\n",
      "0:\tlearn: 0.9953011\ttest: 1.0628243\tbest: 1.0628243 (0)\ttotal: 38.7ms\tremaining: 5m 9s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 1.062314986\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "fold 5: Pearson = 0.05740\n",
      "\n",
      "OOF Pearson (whole series) = 0.03885\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "oof_pred = np.full(len(y_train), np.nan, dtype=np.float32)\n",
    "models   = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(tscv.split(x_train), 1):\n",
    "    x_tr, y_tr = x_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    x_val, y_val = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    train_pool = Pool(x_tr, y_tr)       # cat_cols empty → fine\n",
    "    val_pool   = Pool(x_val, y_val)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=8000,\n",
    "        learning_rate=0.05,\n",
    "        depth=8,\n",
    "        l2_leaf_reg=3,\n",
    "        loss_function=\"RMSE\",           # ← built-in\n",
    "        eval_metric=\"RMSE\",\n",
    "        random_seed=42 + fold,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=300,                    # early-stop patience\n",
    "        verbose=400,\n",
    "        task_type=\"CPU\"\n",
    "    )\n",
    "    model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "    pred_val = model.predict(val_pool)\n",
    "    oof_pred[val_idx] = pred_val\n",
    "\n",
    "    r_fold = pearsonr(y_val, pred_val)[0]\n",
    "    print(f\"fold {fold}: Pearson = {r_fold:.5f}\")\n",
    "\n",
    "    models.append(model)\n",
    "    del train_pool, val_pool, x_tr, x_val\n",
    "    gc.collect()\n",
    "\n",
    "mask = ~np.isnan(oof_pred)\n",
    "oof_r = pearsonr(y_train.iloc[mask], oof_pred[mask])[0]\n",
    "print(f\"\\nOOF Pearson (whole series) = {oof_r:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "706f136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iters = int(np.mean([m.get_best_iteration() for m in models]))\n",
    "full_pool = Pool(x_train, y_train)\n",
    "\n",
    "full_model = CatBoostRegressor(\n",
    "    iterations=best_iters,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    loss_function=\"RMSE\",\n",
    "    random_seed=999,\n",
    "    verbose=False,\n",
    "    task_type=\"CPU\"\n",
    ")\n",
    "full_model.fit(full_pool)\n",
    "\n",
    "preds = full_model.predict(Pool(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb698214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_cat.csv\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(f\"{input_folder}sample_submission.csv\")\n",
    "sample[sample.columns[-1]] = preds.astype(np.float32)\n",
    "sample.to_csv(f\"{output_folder}submission_cat.csv\", index=False)\n",
    "print(\"Saved submission_cat.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drw_crypto_pred_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
